[
  {
    "objectID": "1.Busqueda_bibliografica.html",
    "href": "1.Busqueda_bibliografica.html",
    "title": "Busqueda bibliográfica",
    "section": "",
    "text": "En este documento se describe el proces para obtener la matriz final bibliográfica."
  },
  {
    "objectID": "1.Busqueda_bibliografica.html#busqueda-en-los-motores-de-scopus-e-isi-web-of-science",
    "href": "1.Busqueda_bibliografica.html#busqueda-en-los-motores-de-scopus-e-isi-web-of-science",
    "title": "Busqueda bibliográfica",
    "section": "1 Busqueda en los motores de Scopus e ISI Web of Science",
    "text": "1 Busqueda en los motores de Scopus e ISI Web of Science\n\n1.1 Busqueda del 15/03/2022\nLa busqueda de bibliografia se enfoco a los motores Scopus e ISI Web of Science. La primera busqueda se realizó el 15 de marzo del 2022 usando las siguientes palabras clave\n\nPara SCOPUS:\n\n(seascape AND genomics) (title) OR (marine AND genomics) ABS (year) >2009\nEsta busqueda dio 667 hits\n\nPara ISI:\n\nseacape genomics (title) OR (marine AND genomics) (Abstract) from year >2009\nEsta busqueda arrojo 603 hits\n\n\n1.2 Busqueda del 17/03/2022\nPara incluir estudios que no se pueden considerar como seascape pero que detectaron evidencias de adaptación local o divergencia genética en el ambiente marino, se amplio la busqueda utilizando los siguientes parámetros:\n\nPara SCOPUS\n\n\n\n\n\n\n\n\n\ntitle\nseascape genomics\n\n\n\nOR Author keywords\npopulation genomics OR local adaptation OR rad-seq\n\n\n\nAND All fields\nmarine\n\n\n\nNOT All fields\nmicrobiome OR bacteria\n\n\n\nPublication Date\n2010 - 2022\n\n\n\n\nQue arrojó 616 documentos\n\nPara ISI\n\n\n\n\n\n\n\n\n\ntitle\nseascape genomics\n\n\n\nOR Author keywords\npopulation genomics OR local adaptation OR rad-seq\n\n\n\nAND All fields\nmarine\n\n\n\nNOT All fields\nmicrobiome OR bacteria\n\n\n\nPublication Date\n2010 - 2022\n\n\n\n\nEsto resultó en 520 documentos\n\n\n\n\n\n\nNota\n\n\n\nEvidentemente, hay hits repetidos entre las búsquedas realizadas en ambas fechas. En pasos posteriores se eliminan los duplicados\n\n\n\n\n1.3 Estudios adicionales\nAdemás de las busquedas en Scopus e Isi, se incorporaron estudios adicionales los cuales se utilizaron para construir la propuesta del presente estudio. Estos son:\n\n\n\n\n\n\n\n\n\n\n\n\n\nTitle\nDOI\nAuthors\nYear\nKeywords\ndatabase\n\n\n\n\nParallel adaptive evolution of Atlantic cod on both sides of the Atlantic Ocean in response to temperature\n10.1098/rspb.2010.0985\nBradbury, Ian R.\n2010\natlantic ocean; atlantic cod; local adaptation; natural selection; single nucleotide polymorphysm; temperature\nOther\n\n\nIdentifying patterns of dispersal, connectivity and selection in the sea scallop, Placopecten magellanicus, using RADseq-derived SNPs\n10.1111/eva.12432\nVan Wyngaarden, Mallory\n2017\nradseq; connectivity; dispersal; outlier loci; population genomics; population structure; sea scallop; single nucleotide polymorphysm\nOther\n\n\nRAD sequencing reveals genomewide divergence between independent invasions of the European green crab ( Carcinus maenas ) in the Northwest Atlantic\n10.1002/ece3.2872\nJeffery, Nicholas W.\n2017\ncoi; carcinus maenas; european green crab; population structure; restriction-site-associated dna sequencing\nOther\n\n\nA climate-associated multispecies cryptic cline in the northwest Atlantic.\n10.1126/sciadv.aaq0929\nStanley, Ryan R. E.\n2018\nclimate change; atlantic ocean; temperatura; climatic gradient; spatial genetic structure; genetic structure\nOther\n\n\nParallel adaptive evolution of geographically distant herring populations on both sides of the North Atlantic Ocean\n10.1073/pnas.1617728114\nLamichhaney, Sangeet\n2017\natlantic herring; genetic adaptation; parallel evolution; reproductive strategies; whole-genome sequencing\nOther\n\n\nSeascape genomics reveals adaptive divergence in a connected and commercially important mollusc, the greenlip abalone ( Haliotis laevigata ), along a longitudinal environmental gradient\n10.1111/mec.14526\nSandoval‐Castillo, Jonathan\n2018\nclimate change; ddrad-seq; ecological genomics; landscape genomics; marine protected areas (mpas); population connectivity; southern australia\nOther\n\n\nPopulation genomics reveals a mismatch between management and biological units in green abalone ( Haliotis fulgens )\n10.7717/peerj.9722\nMejía-Ruíz, Paulina\n2020\nabalone; ddrad-seq; fisheries management; green abalone; haliotis fulgens; population genomics; snps\nOther\n\n\nNeutral and adaptive population structure of pink abalone ( Haliotis corrugata ): fishery management implications\n10.1093/icesjms/fsab098\nAlberto Mares-Mayagoitia, Jorge\n2021\nddrad; haliotis corrugata; pink abalone; population genomics\nOther\n\n\nPopulation genomic evidence for adaptive differentiation in Baltic Sea three-spined sticklebacks\n10.1186/s12915-015-0130-8\nGuo, Baocheng\n2015\nbaltic sea; gasterosteus acucelatus; rad-sequencing; snps; local adaptation; population differentiation\nOther"
  },
  {
    "objectID": "1.Busqueda_bibliografica.html#procesamiento-de-las-bases-de-datos",
    "href": "1.Busqueda_bibliografica.html#procesamiento-de-las-bases-de-datos",
    "title": "Busqueda bibliográfica",
    "section": "2 Procesamiento de las bases de datos",
    "text": "2 Procesamiento de las bases de datos\n\n2.1 Caracterización general de cada base de datos\n\n\n\n\n\n\nConsiderando ambas búsquedas bibliográficas se obtuvieron:\n\n1079 resultados en ISI Web\n1252 resultados en Scopus\n9 otros\n\n\n\n\n\n\n\n\n\n\n\n2.2 Pre-filtrado de los registros\nSe filtraron aquellos registros de tipo Article y se eliminaron aquellos relacionados con Bacteria o microbiome\n\n\n\n\n\n\n\n\nTras filtrar estos registro se obtuvieron:\n\n700 resultados en ISI Web\n736 resultados en Scopus\n9 otros\n\n\n\n\nEn total se eliminaron 895 documentos que no son articulos o que estan relacionados con microbioma o bacteria\n\nVenn plot of documents\n\n\n\n\n\n\n\nQuitar duplicados entre todas las bases de datos\n\n\n\n\n\n\n\n\n\n\nCaracterización de las palabras clave en la base de datos\n\n\n\n\n\n\n\n\nTras quitar los duplicados entre las tres bases de datos, quedan 1091 documentos los cuales pasaran a la siguiente etapa de revisión.\nEsta segunda etapa consiste en la revisión manual de los abstracts, asi como las palabras clave y el titulo para identificar.\n\n\n\n2.3 Resultados de la primera ronda de selección\nEn esta ronda se excluyeron los artículos que tuvieran las caracteristicas:\n\nEspecies no marinas (lagos, ríos, etc.)\nEstudios predominantemente teóricos o de revisión\nEstudios predominantemente experimentales (transplante, laboratorio, etc.)\nEstudios enfocados a toxicología o efectos de degradación del hábitat (metales pesados, contaminantes, etc.)\nEstudios que utilizaron unicamente marcadores mitocondriales\nEstudios enfocados predominantemente al ensamblado de genomas o transcriptomas\n\n\n\n\nTras esta primera ronda de selección, quedaron 118\n\n\n2.4 caracterización de las palabras clave de la primera ronda de selección"
  },
  {
    "objectID": "1.Busqueda_bibliografica.html#segunda-ronda-de-selección",
    "href": "1.Busqueda_bibliografica.html#segunda-ronda-de-selección",
    "title": "Busqueda bibliográfica",
    "section": "3 Segunda ronda de selección",
    "text": "3 Segunda ronda de selección\nPara esta ronda, se revisó manualmente los articulos en completo y se seleccionó a aquellos articulos relevantes para el análisis\nTras esta ronda, se seleccionaron XXX"
  },
  {
    "objectID": "1.Busqueda_bibliografica.html#diagrama-de-trabajo",
    "href": "1.Busqueda_bibliografica.html#diagrama-de-trabajo",
    "title": "Busqueda bibliográfica",
    "section": "4 Diagrama de trabajo:",
    "text": "4 Diagrama de trabajo:"
  },
  {
    "objectID": "2.Descripcion_metadatos.html",
    "href": "2.Descripcion_metadatos.html",
    "title": "Descripción de metadatos",
    "section": "",
    "text": "1 Caracterización taxonomica de los metadatos\n\n\n\n\n\n\n\n\n\n\nAlgunos estudios reportan mas de un grupo taxonómico\n\n\n\n\n\n\n\n\n\n\n\n\n2 Caracterización de las regiones estudiadas\nPara cada estudio se clasificaron las regiones oceanograficas\n\n\n\n\n\nLa región Noroeste del Atlántico (North West Atlantic) fue la región con mayor número de sitios de colecta (EEUU y Canadá), seguido del Suroeste del Pacífico (Sout West Pacific) con sitios de colecta en Australia y Nueva Zelanda.\nVisualización de las regiones\n\n\ncharacter(0)\n\n\n\n\n\n\n\n\n\n\n\n\n3 Caracterización de las variables ambientales\nEl principal objetivo de este análisis es identificar si los patrones de conectividad o divergecia genética estan asociadas a varibles oceanograficas. A continuación se describe cuales variables fueron las utilizadas en los estudios analizados\n\n\n\n\n\n\n\n\n\n\nEn la tabla de metadatos se incluyeron estudios en los cuales no se evalúo la asociación con el ambiente, por lo que varios de los estudios no contienen información ambiental\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMuchos de los estudios descritos en los metadatos incluyen también variables geográficas (latitud, longitud, profundidad) o corrientes oceánicas. Sin embargo, para esta descripción preliminar solo nos enfocamos a las variables fisico-quimicas y biológicas.\n\n\n\n\n\n\n\nGráfico responsivo\n\n\n\n\n\n\n\n\n4 Asociación de divergencía genética con el ambiente\nA continuación se muestra cual fue el principal factor ambiental asociado a divergencia genética en los estudios donde se detectó asociación con parámetros ambientales\n\n\n\n\n\n\n\n\n\n\nDependiendo del método utilizado por los autores para evaluar el principal factor ambiental, algunos estudios presentan uno o mas main drivers. Por lo tanto, se incorporó un nudge a los puntos en el mapa y evitar que se translapen entre ellos.\n\n\n\n\n\n\n\nMapa responsivo"
  },
  {
    "objectID": "3.Caraterizacion_ambiental.html",
    "href": "3.Caraterizacion_ambiental.html",
    "title": "Caracterización ambiental",
    "section": "",
    "text": "A partir de la tabla de metadatos, se seleccionaran estudios y regiones oceanográficas de transición. Estas se seleccionarion con base a los siguientes criterios:\n\nHay evidencia multi-especie de divergencia genética\nHay evidencia de que esta divergencia genética esta asociada a variables ambientales\nExiste información disponible (genotipos) para probar estas evidencias\n\nLos estudios y regiones seleccionadas fueron:\n\n\n\nLista de estudios y regiones de transición\n\n\n\n\n\n\n\n\n\n\n\nDOI_ID\nSpecies\nGeneric\nWorld region\nCountry\nEnvironmental association\nMain environemntal driver adaptive\n\n\n\n\ns12862-020-01679-4(1)\nScutellastra granularis\nGastropod\nSouth Africa\nSouth Africa\nyes\nMean sea surface temperature\n\n\nmec.14526\nHaliotis laevigata\nGastropod\nSouth West Pacific\nAustralia\nyes\nMinimum sea surface temperature; Oxygen concentration\n\n\nmec.13811\nNA\nCrustacean\nNorth West Atlantic\nCanada\nyes\nMinimum sea surface temperature\n\n\neva.12932\nNA\nBivalve\nNorth East Atlantic\nIreland\nyes\nMinimum sea surface temperature in April\n\n\nrspb.2021.0407\nAmphiprion clarkii\nFish\nIndo West Pacific\nJapan; Philipines; Indonesia\nYes\nAnnual mean sea surface temperature; Annual minimum sea surface temperature\n\n\ns12864-020-07084-x\nPenaeus monodon\nCrustacean\nSouth West Pacific\nAustralia\nYes\nSurfce temperature maximun; Surface temperature minimun\n\n\nmec.15499\nMallotus villosus\nFish\nNorth West Atlantic\nCanada\nYes\nSea surface temperature; Chlorophyll concentration; Spawning habitat\n\n\neva.12905\nPinctada fucata\nBivalve\nNorth West Pacific\nJapan, China, Cambodia, Myanmar\nYes\nMean sea surface temperature\n\n\nece3.3846\nPlacopecten magellanicu\nBivalve\nNorth West Atlantic\nUS; Canada\nYes\nMinimum sea surface temperature; Deep average winter temperature\n\n\nfsx160\nSolea solea\nFish\nNorth East Atlantic\nUK; France; Spani; Portugal;Netherlands; Denmark; Norway; Sweeden\nyes\ntemperature in winter\n\n\npeerj.9722\nHaliotis fulgens\nGastropod\nEast Pacific\nMexico\nNA\nNA\n\n\nfsab098\nHaliotis corrugata\nGastropod\nEast Pacific\nMexico\nyes\nAnual range sea surface temperature\n\n\nmec.15128\nHaliotis rubra\nGastropod\nSouth West Pacific\nAustralia\nYes\nSummer sea surface temperature\n\n\nmec.16371\nCarcinus maenas\nCrustacean\nNorth East Atlantic\nFrance; Netherlands; Germany; Denmark; Sweden\nYes\nTidal cycles\n\n\neva.12601\nCarcinus maenas\nCrustacean\nNorth West Atlantic\nUS; Canada\nYes\nWinter Sea surface temperature"
  },
  {
    "objectID": "3.Caraterizacion_ambiental.html#variables-ambientales",
    "href": "3.Caraterizacion_ambiental.html#variables-ambientales",
    "title": "Caracterización ambiental",
    "section": "2 Variables ambientales",
    "text": "2 Variables ambientales\nA continuación se muestra los sitios de muestreo en cada una de las regiones.\n\nLas variables oceanograficas se descargaron de CMEMS utilizando la base de datos GLOBAL_REANALYSIS_PHY_001_031 para temperatura y salinidad, y GLOBAL_MULTIYEAR_BGC_001_029 para clorodila A.\nEn todos los casos, se descargaron datos mensuales de cada región.\n\n\n\n\n\n\n\n\n\n\nSobre kurtosis\n\n\n\nLa curtosis es una estadística que determina el grado de concentración de los valores de una variable en torno al centro de la distribución de frecuencias.\nUna distribución normal tiene un valor de curtosis de 3\n\n\n\n\n\nnull device \n          1 \n\n\n\n\n\n\n\n\nTemperaturaSalinidadClorofila A"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "databases/genomic_repositories/rspb.2021.0407/BayPass_upstream.html",
    "href": "databases/genomic_repositories/rspb.2021.0407/BayPass_upstream.html",
    "title": "Seascape2022",
    "section": "",
    "text": "Code for BayPass analyses run on an HPC cluster (Amarel at Rutgers) before transferring output files to local computer for downstream analyses and visualization in R.\nDownloaded BAYPASS v.2.1 to Amarel and unzipped/installed in ~/Programs.\nNote: Process below is for running BAYPASS with the full mac2 dataset (wN4). For the dataset w/out N4, inputs and file names were changed as needed.\nCreated BAYPASS input file with PGDSpider v.2.1.1.5 & modifications on local computer.\n\nTook output.hicov2.snps.only.mac2.vcf file and converted to clownfish_mac2.snps.\n\nUsed following code to run PGDSpider v.2.1.1.5 on Amarel:\n#grabbed an interactive node with following line of code\nsrun --partition=main --nodes=1 --ntasks=1 --cpus-per-task=1 --mem=2000 --time=00:30:00 --export=ALL --pty bash -i\n\n#called #PGDSpider in ~/Programs/PGDSpider_2.1.1.5\njava -Xmx1024m -Xms512m -jar PGDSpider2-cli.jar -inputfile output.hicov2.snps.only.mac2.vcf -inputformat VCF -outputfile clownfish_mac2.snps -outputformat BAYENV -spid VCF_BAYENV.spid\n\n#NOTE: if don't have spid file made, or know proper format, running the previous line of code without the -spid argument will generate a template spid file to modify as needed\nSpid format for PGDSpider:\n# VCF Parser questions\nPARSER_FORMAT=VCF\n\n# Only output SNPs with a phred-scaled quality of at least:\nVCF_PARSER_QUAL_QUESTION=\n# Select population definition file:\nVCF_PARSER_POP_FILE_QUESTION= Pop_assignments.txt #txt file with indv names in one column and pop assignment (Pop_1, etc.) in next (no headers, tab-delimited)\n# What is the ploidy of the data?\nVCF_PARSER_PLOIDY_QUESTION=  DIPLOID\n# Do you want to include a file with population definitions?\nVCF_PARSER_POP_QUESTION= TRUE\n# Output genotypes as missing if the phred-scale genotype quality is below:\nVCF_PARSER_GTQUAL_QUESTION=\n# Do you want to include non-polymorphic SNPs?\nVCF_PARSER_MONOMORPHIC_QUESTION= TRUE\n# Only output following individuals (ind1, ind2, ind4, ...):\nVCF_PARSER_IND_QUESTION=\n# Only input following regions (refSeqName:start:end, multiple regions: whitespace separated):\nVCF_PARSER_REGION_QUESTION=\n# Output genotypes as missing if the read depth of a position for the sample is below:\nVCF_PARSER_READ_QUESTION=\n# Take most likely genotype if \"PL\" or \"GL\" is given in the genotype field?\nVCF_PARSER_PL_QUESTION= FALSE\n# Do you want to exclude loci with only missing data?\nVCF_PARSER_EXC_MISSING_LOCI_QUESTION= FALSE\n\n# BAYENV Writer questions\nWRITER_FORMAT=BAYENV\n\n# Save sample file\nBAYENV_WRITER_SAMPLE_FILE_QUESTION=\n# Assign half missing genotypes (one allele missing) as complete missing?\nBAYENV_WRITER_HALF_MISSING_QUESTION=\n# Do you want to save two additional files with used sample and loci names?\nBAYENV_WRITER_WRITE_INFO_FILE_QUESTION=\n# Do you want to save an additional sample file with sample sizes?\nBAYENV_WRITER_WRITE_SAMPLE_FILE_QUESTION=\n# Save sample/loci names file\nBAYENV_WRITER_INFO_FILE_QUESTION=\nCurrently, there is no option to convert a VCF file (or any other file format) to the input file format BAYPASS needs. Best option is to create input file for BayEnv (similar program to BAYPASS) and then convert to BAYPASS using either regex or on local computer.\n\nTo convert to BAYPASS input, need to turn 3 columns (each pop has 1 column) into 6 columns (each pop has 2 columns). BayEnv input file has every allele as one row (e.g., the first row is the count for the first allele at the first SNP in all three pops, the second row is the cound for the second allele at the first SNP, etc.). The BayPass input file has every SNP as one row (e.g., the first row is the count for the first and second allele at the first SNP in all three pops - thus the 2 columns for each pop).\n\nTransformed BayEnv input file to BAYPASS input file titled clownfish_mac2.geno.\nNext, standardized env variables of interest (SST mean, SST min, SST max, lat, SSS mean). All env variables (except for latitude) were pulled from the MARSPEC GIS database for each sampling location.\nTo standardize, ran following code in R:\n#example for SSS mean\n\n#create vector with raw data\nSSS_mean_raw <- c(34.38, 34.09, 33.66)\nSSS_mean_st <- scale(SSS_mean_raw)\n\n#done for each env of interest\n#NOTE: SST min & SST max were put in one vector and standardized together because they use same original scale (degrees celsius)\nOnce standardized, values put into a separate *.env text file for each env variable (3 columns, 1 row - each pop in own column in same order as in *.geno file).\n\nEx: clownfish_data_ssmean.env\n\nCreated BayPass.sbatch file to run BAYPASS:\n#!/bin/bash\n\n#SBATCH --partition=p_mlp195 #p_mlp195 for pinsky lab partition, otherwise use main (or EOAS or E&E)\n#SBATCH --job-name=baypass\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=8000\n#SBATCH --time=2:00:00\n\n#SBATCH --requeue\n#SBATCH --export=ALL\n#SBATCH --output=slurm.%j.out\n#SBATCH --error=slurm.%j.err\n#SBATCH --mail-type=END\n#SBATCH --mail-user=rene.clark@rutgers.edu\n\n#script for running BayPass on Amarel\n\ncd ~/Aclarkii_Transcriptome/BayPass\n\n#uncomment if need to run core model to create covariance matrix and standardize env variables\ng_baypass -npop 3 -gfile clownfish_mac2.geno -outprefix mac2core1\n\n#uncomment if need to run auxiliary model (make sure env variables are standardized!)\ng_baypass -npop 3 -gfile clownfish_mac2.geno -efile clownfish_data_sssmean.env -auxmodel -omegafile clownfish_mat.cov -outprefix mac2aux1_ssmean\nRan with default settings: burn-in of 5,000 iterations followed by an additional 25,000 MCMC steps with thinning every 25 iterations (for more info/options, call g_baypass -help).\nFirst, ran just core model to (1) create covariance matrix for the standard covariate model (aux model) and (2) generate XtX statistics. BAYPASS generates many output files, but the useful ones here are:\n\nmac2core1_mat_omega.out: This is the covariance matrix needed to run aux model (file ending needs to be changed to *.cov.)\nmac2core1_summary_pi_xtx.out: This has the XtX variables for each SNP.\n\nThen, ran the axuiliary model to generate Bayes Factors for each env variable/SNP combination. Ran model for each env variable separately, to avoid issues with correlation. Again, got a lot of output files, but the *_summary_betai.out files contain the Bayes Factors.\n\nRan 2X for each env variable to make sure model converged (got same Bayes Factors each time).\n\nCopied *_summary_betai.out files to local computer and read into R for downstream analyses (Scripts/Pull_BFs.R).\nTo create XtX threshold value\nAfter the original core model was run (with raw data), copied clownfish_mac2_mat.cov, mac2core1_summary_beta_params.out, and clownfish_mac2.geno to local computer and read into R (Scripts/XtX_Calibration.R) to generate pseudo-observed data sets (PODS) under a null model of no selection.\n\nUsed functions from baypass_utils.R script that comes with BAYPASS download/installation (found in baypass_2.1/utils).\n\nTook *.geno file that was generated using the simulate.baypass() function and re-ran core model in BAYPASS with that as input. Then, copied the *_summary_pi_xtx.out file created from that BAYPASS run, and copied it to local computer. Read into R, and used the XtX values in the file to generate an empirical distribution of XtX under no selection. Used the 99% quantile of this distribution as the selection/neutrality threshold. SNPs with XtX values above this cut-off (from raw data) are considered adaptively differentiated.\n\nCode for this in Scripts/XtX_Calibration.R.\n\nTo assess SNP-environment associations\nTo determine if we had more significant SNP-environmental associations than expected by chance, we created ten permuted datasets to represent the null hypothesis of no associatio nbetween allele frequencies and environmental conditions. These datasets by randomly reassigning individuals to one of the three sampling locations (Scripts/Write Simulation Pop Data for BayPass.R –> creates BayEnv-formatted file which needs to be changed to BAYPASS input format).\nOnce input files were created, ran the permuted datasets through BAYPASS in the same manner as the raw data (first ran core model to create covariance matrix, then ran auxiliary model separately for each env variable).\nCopied the *_summary_betai.out files to local computer, and calculated the mean Bayes Factor for each unique SSNP-environmental covariable combination across all ten permuted datasets to create a null distribution of BFs to compare wit the resutls from the raw data. These values were combined with the Bayes Factors from the raw data in Data/M-W_test_BF_data.csv, which was read into Scripts/ECDFS for Sim v Real BFs.R to compare the two distributions."
  },
  {
    "objectID": "databases/genomic_repositories/rspb.2021.0407/Fastsimcoal2_upstream.html",
    "href": "databases/genomic_repositories/rspb.2021.0407/Fastsimcoal2_upstream.html",
    "title": "Seascape2022",
    "section": "",
    "text": "Code for fastsimcoal2 analyses run on an HPC cluster (Amarel at Rutgers) before transferring output files to local computer for downstream analyses.\nNote: A lot of this code was first written by Jennifer Hoey, and modified for this project. For more information, check out her repo: https://github.com/pinskylab/NePADE/blob/master/demo_modeling/fsc_models/. \nFor fastsimcoal2, had to first generate the observed multidimensional site frequency spectrum (MSFS) with easySFS.py from (https://github.com/isaacovercast/easySFS). To main neutrality and retain rare variants, built the observed MSFS with a dataset unfiltered for minor allele counts but without outlier SNPs (5,662 SNPs). All individuals were included.\nTo generate MSFS\nOpened up conda on Amarel and created environment for easySFS:\nmodule use /projects/community/modulefiles #to access community modules not in core set\nmodule load py/data-science/stack/5.1.0-kp807 #module with anaconda\n\nconda create -n easySFS #create easySFS environment\nconda config --add channels bioconda #channel with bioinformatics packages\nconda install -c bioconda dadi pandas #add dadi & pandas packages\n\n#downloaded easySFS repo from GitHub and unzipped in ~/Programs directory\nunzip easySFS-master.zip\nmv easySFS-master easySFS\ncd easySFS\nchmod 777 easySFS.py #to make executable\nCopied appropriate VCF file (output.hicov2.snps.only.mac1.nooutliersfinal.reordered.vcf) to ~/Programs/easySFS directory. This VCF was made by filtering original vcf output.hicov2.snps.only.vcf to keep only SNPs that were polymorphic (had at least 1 individual with at least 1 minor allele) and were not outlier SNPs. The individuals were also re-ordered to match the geographic order (Japan, Philippines, Indonesia). Filtering was done with VCFtools.\nPreviewed SFS with following code:\nconda activate easySFS\n\n./easySFS.py -i output.hicov2.snps.only.mac1.nooutliersfinal.reordered.vcf -p popfile.txt --preview -a \n\n#popfile.txt is a 2 column, tab-separated file with individuals in first column and pop name in second. \n#Sometimes need to recreate this as doesn't always properly read all populations.\n\n#NOTE: -a flag forces easySFS to keep all SNPs within each RAD locus (doesn't randomly sample 1 SNP per locus)\nPreview shows # segregating sites depending on # individuals sampled in each pop.\n\nBecause, if missing sites, downsampling can increase # of segregating sites. Since we don’t have missing data, used full dataset (all individuals).\n\nCreated SFS with following code:\n./easySFS.py -i output.hicov2.snps.only.mac1.nooutliersfinal.reordered.vcf -p popfile.txt --proj 16,20,14 -a -o output -f\nAbove code creastes all possible SF (each pop, 2D-joint SFS and MSFS) formatted for input into dadi and fastsimcoal2. Used the MSFS for fastsimcoal2 (output_MSFS.obs renamed to apcl_MSFS.obs).\nTo run fastsimcoal2\nFirst, specified the model with the *.tpl and *.est files. The *.tpl file contains the model structure (sampling scheme information, number/types of demographic events, etc.). The *.est file contains the prior distributions for each parameter estimated in the model.\n\nNOTE: It’s important to keep naming consistent! All of these files (along with the *.obs file) should have the same prefix, so fastsimcoal2 knows what to read in.\n\nThe apcl.tpl file (also included in the Data directory):\n#Any place where there is a word is a parameter that fsc is going to estimate\n#Any place where there is a number is a set parameter\n#fsc works BACKWARDS in time (coalescent model). So the first demographic event happened most recently in time, then moves backwards from there\n\n//Amphiprion connectivity: Japan, Philippines, Indonesia\n3 samples\n//Population effective sizes (=2N, number of alleles)\nPOPONE\nPOPTWO\nPOPTHREE\n//Sample sizes: # samples (=2N, number of alleles)\n16\n20\n14\n//Growth rates\n0\n0\n0\n//Number of migration matrices : If 0 : No migration between demes\n3\n//migration matrix 0: sources in rows, sinks in columns (backwards in time)\n0 DISPONE 0\nDISPONE 0 DISPTWO\n0 DISPTWO 0\n//migration matrix 1: At 3600 gen (when J & P merge to P), migration only P-I\n0 0 0\n0 0 DISPTWO\n0 DISPTWO 0\n//migration matrix 2: At TDIVTWO (when P & I merge to I), no migration\n0 0 0\n0 0 0\n0 0 0\n//Historical event format: time, src, sink, % mig, new Ne scaling factor for sink, new r, new mig matrix:\n2 historical events\n3600 0 1 1 1 0 1\nTDIVTWO 1 2 1 1 0 2\n//Number of chromosomes\n1 0\n//Number of linkage blocks on Chromosome 1\n1\n//Per Block: Data type, No. of loci, Recombination rate to the right-side locus, plus optional parameters\nFREQ 1 0 1E-8 OUTEXP\nThe apcl.est file (also included in the Data directory):\n// Priors and rules file\n// *********************\n\n[PARAMETERS]\n//#isInt?       #name   #dist.  #min    #max\n//all N are in number of haploid individuals\n1 POPONE logunif 100 100000 output\n1 POPTWO logunif 100 100000 output\n1 POPTHREE logunif 100 100000 output\n0 DISPONE unif 0 0.3 output\n0 DISPTWO unif 0 0.3 output\n1 TDIVTWO unif 3600 6000 output\n\n[RULES]\n\n[COMPLEX PARAMETERS]\nOnce the .tpl, .est, and MSFS files were created, downloaded fastsimcoal26 to Amarel and unzipped/installed in ~/Programs.\nwget http://cmpg.unibe.ch/software/fastsimcoal2/downloads/fsc26_linux64.zip\nunzip fsc26_linux64.zip\ncd fsc26_linux64\nchmod 777 fsc26 #make executable\n\n#copied .tpl, .est, and .obs files to this fsc26_linux64 as well\nCreated a directory in /scratch to read, write, and execute fsc jobs from (should go faster and don’t have to worry about storage space in home directory).\nCreated run_fscmodel_singlethread.sbatch script for fastsimcoal2 to perform 100,000 coalescent simulations to estimate the SFS, with 40 optimization (ECM) cycles. This script was modified from https://github.com/pinskylab/NePADE/blob/master/demo_modeling/fsc_models/run_model6_singlethread.sh.\n\nThis script sets up file structure that will be important for gleaning information from the replicate runs and easily grabbing maximum likelihoods and parameter estimates. Briefly, it makes a slurm-out directory that contains the output from each replicate run. In this directory, are the same number of directories as replicate runs, each named by the slurm job id the run was submitted under. The script then copies all input files into each slurm-out/SLURM_JOBID directory and runs fastsimcoal2 from there each time.\n\nIt also writes several settings files that help with troubleshooting.\n\n\n#!/bin/bash\n#SBATCH --job-name=fsc\n#SBATCH --partition=p_mlp195,main #p_mlp195 for pinsky lab partition, otherwise use main (or EOAS or E&E)\n#SBATCH -N1\n#SBATCH -n1                        #28 cores on amarel\n#SBATCH --cpus-per-task=1         #28 cores on amarel\n############################\n#SBATCH --mem=5000               #128GB /node of amarel\n#SBATCH --time=10:00:00            #max time is 3 days:  3-00:00:00  or  72:00:00\n#SBATCH --export=ALL\n#SBATCH --requeue       #optional: this prevent the job from restarting if the node fails or the jobs is preempted.\n#SBATCH -o /scratch/rdc129/Aclarkii_Demo/slurm-%j.out   ## the default file name is \"slurm-%j.out\"\n#SBATCH -e /scratch/rdc129/Aclarkii_Demo/slurm-%j.err\n#SBATCH --mail-type=END                # Type of email notification- BEGIN,END,FAIL,ALL\n#SBATCH --mail-user=rene.clark@rutgers.edu # Email to which notifications will be sent\n\n#############################################################\n# standard output is saved in a file:  slurm-$SLURM_JOBID.out\n#############################################################\n\nmkdir -p /scratch/$USER/Aclarkii_Demo/slurm-out/$SLURM_JOBID\ncp /home/$USER/Programs/fsc26_linux64/fsc26 /scratch/$USER/Aclarkii_Demo/apcl.tpl /scratch/$USER/Aclarkii_Demo/apcl.est /scratch/$USER/Aclarkii_Demo/apcl_MSFS.obs /scratch/$USER/Aclarkii_Demo/slurm-out/$SLURM_JOBID\ncd /scratch/$USER/Aclarkii_Demo/slurm-out/$SLURM_JOBID\n\n#create a personal dirctory on the node scratch disk if needed.\n#mkdir -p /mnt/scratch/$USER/$SLURM_JOBID\n\n#this will tell you when the job started and the host.\ndate=`date \"+%Y.%m.%d-%H.%M.%S\"`\nhostname=`hostname`\n\n#to print the variable -- echo\necho $date\necho $hostname\n\n#obtain the environment variables\n#this is useful for reference and troubleshooting issues.\nenv >               /scratch/$USER/Aclarkii_Demo/slurm-out/$SLURM_JOBID/slurm-$SLURM_JOBID-env-all.out\nenv | grep SLURM >  /scratch/$USER/Aclarkii_Demo/slurm-out/$SLURM_JOBID/slurm-$SLURM_JOBID-env-SLURM.out\n\n#start time\ndate\n\n#start the simulation\nsrun /home/rdc129/Programs/fsc26_linux64/fsc26 -t apcl.tpl -e apcl.est -n 100000 -m -u -M -L 40 -0 --numBatches 1 --cores 1 > /scratch/$USER/Aclarkii_Demo/slurm-out/$SLURM_JOBID/$SLURM_JOBID-fastsimcoal26.$SLURM_NNODES.$SLURM_NODELIST.$date.a.txt\n\n#end time\ndate\nOnce the model was running well, wrote a shell script (run_sbatch.sb) to submit the previous script many times in a job array (need multiple replicate runs to pick best parameter estimates from). This script was modified from https://github.com/pinskylab/NePADE/blob/master/demo_modeling/fsc_models/run_sbatch_Ne.sh.\n#submits previous script 50x\n\n#!/bin/bash\n#SBATCH --partition=p_mlp195,main\n\nfor i in {1..50}\ndo\nsbatch run_fscmodel_singlethread.sbatch\nsleep 1\n\ndone\nOnce all 50 replicate runs finished running, wrote cat_ML.sbatch script to concatenate the results (.bestlhoods files) into a single file.\n\nThis script was modified from https://github.com/pinskylab/NePADE/blob/master/demo_modeling/fsc_models/cat_bestlhoods.\n\n#!/bin/bash\n\nfor file in /scratch/$USER/Aclarkii_Demo/slurm-out/*/apcl/apcl.bestlhoods\ndo\nsed '1d' \"$file\" > \"$file.trimmed\"\n\ncat /scratch/$USER/Aclarkii_Demo/slurm-out/*/apcl/apcl.bestlhoods.trimmed > /scratch/$USER/Aclarkii_Demo/apcl_raw.bestlhoods\n\ndone\nCopied apcl_raw.bestlhoods file to local computer and read into Excel to sort by maximum likelihood (ML) and find the parameters from the best ML run. These are the best point estimates.\nTo create 95% CIs\nTo create the 95% CIs, first had to create 100 bootstrapped SFS from the raw SFS to run in fastsimcoal2. Used the Bootstrap_forSFS.R script to bootstrap output.hicov2.snps.only.mac1.nooutliersfinal.reordered.vcf 100X. Then used easySFS to create MSFS from each of the bootstrapped VCFs (with the same code as was used to create the original MSFS).\nMoved bootstrapped MSFS to fastsimcoal2 /scratch directory. Each MSFS was placed in its own directory, all of which were placed in a bootstrapSFS directory (ex. the first bootstrapped MSFS was placed in /scratch/rdc129/Aclarkii_Demo/bootstrapSFS/apcl_boot1).\nWrote run_fscmodel_CI_singlethread.sbatch script to run fastsimcoal2 for each of the bootstrapped directories. This script was modified from https://github.com/pinskylab/NePADE/blob/master/demo_modeling/fsc_models/run_fsc_Ne_CI_singlethread.sh.\n\nThe structure of this script is very similar to the one for the original MSFS (creates slurm-out/SLURM_JOBID directories in each bootstrapped directory). One major difference is this script uses the parameter values from the best-fit model with the observed dataset as the starting values for each replicate run (to speed things up). This is the apcl.pv file from the best-fit model, and should be copied to the starting directory (here, scratch/rdc129/Aclarkii_Demo) before running the script.\nTo run the script, need dir_names.txt file, which is a list of all the bootstrapped SFS directory names (apcl_boot1, apcl_boot2, apcl_boot3, etc.).\n\n#!/bin/bash\n#SBATCH --job-name=fsc_boot\n#SBATCH --partition=p_mlp195,p_eoas_1,p_deenr_1,main #p_mlp195 for pinsky lab partition, p_eoas_1 for EOAS partition, p_deenr_1 for E&E partition\n#SBATCH -N1\n#SBATCH -n1                        #28 cores on amarel\n#SBATCH --cpus-per-task=1         #28 cores on amarel\n############################\n#SBATCH --mem=5000               #128GB /node of amarel\n#SBATCH --time=33:00:00            #max time is 3 days:  3-00:00:00  or  72:00:00\n#SBATCH --export=ALL\n#SBATCH --array=1-100\n#SBATCH --requeue       #optional: this prevent the job from restarting if the node fails or the jobs is preempted.\n#SBATCH -o /scratch/rdc129/Aclarkii_Demo/slurm-%A_%a.out   #%A is the jobid and %a is the array index\n#SBATCH -e /scratch/rdc129/Aclarkii_Demo/slurm-%A_%a.err   #/dev/null\n#SBATCH --mail-type=END                #Type of email notification- BEGIN,END,FAIL,ALL\n#SBATCH --mail-user=rene.clark@rutgers.edu #Email to which notifications will be sent\n\n#############################################################\n# standard output is saved in a file:  slurm-$SLURM_JOBID.out\n#############################################################\n\necho \"Starting task $SLURM_ARRAY_TASK_ID\"\nDIR=$(sed -n \"${SLURM_ARRAY_TASK_ID}p\" dir_names.txt)\ncd $DIR\n\nmkdir -p /scratch/$USER/Aclarkii_Demo/bootstrapSFS/$DIR/slurm-out/$SLURM_JOBID\ncp /home/$USER/Programs/fsc26_linux64/fsc26 /scratch/$USER/Aclarkii_Demo/bootstrapSFS/$DIR/slurm-out/$SLURM_JOBID\ncp /scratch/$USER/Aclarkii_Demo/apcl.tpl /scratch/$USER/Aclarkii_Demo/bootstrapSFS/$DIR/slurm-out/$SLURM_JOBID/apcl_boot.tpl\ncp /scratch/$USER/Aclarkii_Demo/apcl.est /scratch/$USER/Aclarkii_Demo/bootstrapSFS/$DIR/slurm-out/$SLURM_JOBID/apcl_boot.est\ncp /scratch/$USER/Aclarkii_Demo/bootstrapSFS/$DIR/apcl_boot_MSFS.obs /scratch/$USER/Aclarkii_Demo/bootstrapSFS/$DIR/slurm-out/$SLURM_JOBID\ncp /scratch/$USER/Aclarkii_Demo/apcl.pv /scratch/$USER/Aclarkii_Demo/bootstrapSFS/$DIR/slurm-out/$SLURM_JOBID/apcl_boot.pv\ncd /scratch/$USER/Aclarkii_Demo/bootstrapSFS/$DIR/slurm-out/$SLURM_JOBID\n\n#Create a personal dirctory on the node scratch disk if needed.\n#mkdir -p /mnt/scratch/$USER/$SLURM_JOBID\n\n\n#this will tell you when the job started and the host.\ndate=`date \"+%Y.%m.%d-%H.%M.%S\"`\nhostname=`hostname`\n\n#to print the variable -- echo\necho $date\necho $hostname\n\n#obtain the environment variables\n#this is useful for reference and troubleshooting issues.\nenv >               /scratch/$USER/Aclarkii_Demo/bootstrapSFS/$DIR/slurm-out/$SLURM_JOBID/slurm-$SLURM_JOBID-env-all.out\nenv | grep SLURM >  /scratch/$USER/Aclarkii_Demo/bootstrapSFS/$DIR/slurm-out/$SLURM_JOBID/slurm-$SLURM_JOBID-env-SLURM.out\n\n\n#start time\ndate\n\n#start the simulation\nsrun ./fsc26 -t apcl_boot.tpl -e apcl_boot.est -n 100000 -m -u -M -L 40 -0 --numBatches 1 --cores 1 --initValues apcl_boot.pv  > /scratch/$USER/Aclarkii_Demo/bootstrapSFS/$DIR/slurm-out/$SLURM_JOBID/$SLURM_JOBID-fastsimcoal26.$SLURM_NNODES.$SLURM_NODELIST.$date.a.txt\nOnce the model was running well, modified run_sbatch.sb to submit the previous script 20 times in a job array (need multiple replicate runs to pick best parameter estimates from).\n\nThis results in 20 replicate runs for each bootstrapped MSFS.\n\nOnce all replicate runs for the bootstrapped data finished, wrote cat_CIs.sbatch script to concatenate the results (.bestlhoods files) from each bootstrapped MSFS into a single file.\n\nThis script was modified from https://github.com/pinskylab/NePADE/blob/master/demo_modeling/fsc_models/cat_cis.\n\n#!/bin/bash\n\ncat /scratch/rdc129/Aclarkii_Demo/bootstrapSFS/apcl_boot1/slurm-out/*/apcl_boot/apcl_boot.bestlhoods.trimmed > /scratch/rdc129/Aclarkii_Demo/bootstrapSFS/apcl_boot1/cis_sfs_summary\n\n#repeat lines for each boot directory (apcl_boot1:apcl_boot100)\nThen, wrote cat_max_summary_CIs.sbatch script to find the parameters from the ML run for each of the 100 bootstrapped MSFS and concatenate into fsc_maxLhood_CI_summary.txt.\n\nThis script was modified from https://github.com/pinskylab/NePADE/blob/master/demo_modeling/fsc_models/cat_max_summary.\n\n#!/bin/bash\n\nfor file in /scratch/rdc129/Aclarkii_Demo/bootstrapSFS/*/cis_sfs_summary\ndo\n\n#Sorts the file by column 6 (likelihood) in descending order and then prints the last row (maximum since it's negative)\nsort -nk 7 \"$file\" | tail -n 1 > \"$file.max\"\n\n#Concatenates the run with the maximum likelihood for each SFS into a file\ncat /scratch/rdc129/Aclarkii_Demo/bootstrapSFS/*/cis_sfs_summary.max > /scratch/rdc129/Aclarkii_Demo/bootstrapSFS/fsc_maxLhood_CI_summary.txt\n\ndone\nCopied fsc_maxLhood_CI_summary.txt file to local computer, converted to .csv, and read into R to calculate 95% CIs with fsc_CIs.R."
  },
  {
    "objectID": "databases/genomic_repositories/rspb.2021.0407/PCA_upstream.html",
    "href": "databases/genomic_repositories/rspb.2021.0407/PCA_upstream.html",
    "title": "Seascape2022",
    "section": "",
    "text": "Code for generating PCAs (eigenvalues & eigenvectors) using plink. Run on an HPC cluster (Amarel at Rutgers) before transferring output files to local computer for downstream analyses and visualization in R.\nDownloaded plink v.1.9 to Amarel and installed in ~/Programs.\nNote: Process below is for creating eigenvalues/vectors with the full mac2 dataset. For the other datasets, inputs and file names were changed as needed.\nCreated PCA.sbatch file to run plink for PCAs:\n\n#!/bin/bash\n\n#SBATCH --partition=p_mlp195 #p_mlp195 for pinsky lab partition, otherwise use main (or EOAS or E&E)\n#SBATCH --job-name=PCA\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=8000\n#SBATCH --time=2:00:00\n\n#SBATCH --requeue\n#SBATCH --export=ALL\n#SBATCH --output=slurm.%j.out\n#SBATCH --error=slurm.%j.err\n#SBATCH --mail-type=END\n#SBATCH --mail-user=rene.clark@rutgers.edu\n\n#script for running Plink1.9 for PCA on Amarel\n#NOTE: probably don't need to sbatch, runs very quickly (either on log-in node or interactive node)\n\ncd ~/Aclarkii_Transcriptome/PCA\n\nplink --pca --allow-extra-chr --vcf output.hicov2.snps.only.mac2.vcf --out output.hicov2.snps.only.mac2\nPlink creates 4 files:\n\n*.eigenvec: Contains eigenvectors\n*.eigenval: Contains eigenvalues\n*.log: Contains information on # variants and individuals used in analysis\n*.nosex: Not relevant\n\nRepeated process for full mac2 dataset, only SNPs in HWE, only non-outlier SNPS, and only outlier SNPs.\nCopied *.eigenvec & *.eigenval files to local computer and opened in Excel to create .csv files that were then read into R for downstream analyses (Scripts/PCAs.R)."
  },
  {
    "objectID": "databases/genomic_repositories/rspb.2021.0407/pi_upstream.html",
    "href": "databases/genomic_repositories/rspb.2021.0407/pi_upstream.html",
    "title": "Seascape2022",
    "section": "",
    "text": "Code for calculating per-site pi using VCFtools. Run on an HPC cluster (Amarel at Rutgers) before transferring output files to local computer for downstream analyses and visualization in R.\nVCFtools v.1.16 was already installed as a module on Amarel. To load in working environment, use following code:\nmodule load VCFtools\nFirst, created sub-setted VCF files for each sampling site (code for Japan below):\n#created J_individuals.txt (column of individuals in Japan)\n\n#used VCFtools to filter output.hicov2.snps.only.mac2.vcf to only include individuals from Japan\nvcftools --vcf output.hicov2.snps.only.mac2.vcf --keep J_individuals.txt --recode --recode-INFO-all --out Jmac2\n\n#repeated for Philippines & Indonesia\nCalculated site pi, both within each population and with the full dataset (all individuals included in VCF). Also calculate with only synonymous SNPs (see TajimasD_upstream.md for more information on how SNPs were identified):\n#NOTE: don't grab interactive node for VCFtools jobs bc run in seconds, BUT if is more data-intensive, should get off log-in node to do this.\n\nvcftools --vcf Jmac2.vcf --site-pi --out Jmac2\n\n#repeated for Philippines, Indonesia, and the original vcf file w/all individuals\nCopied *sites.pi files to local computer and opened in Excel to create .csv files that were then read into R for downstream analyses (Scripts/pi.R)."
  },
  {
    "objectID": "databases/genomic_repositories/rspb.2021.0407/StairwayPlot_upstream.html",
    "href": "databases/genomic_repositories/rspb.2021.0407/StairwayPlot_upstream.html",
    "title": "Seascape2022",
    "section": "",
    "text": "Code for Stairway Plot v.2 analyses run on an HPC cluster (Amarel at Rutgers) before transferring output files to local computer for downstream analyses.\nFor Stairway Plot, had to first generate the observed folded site frequency spectrum (SFS). To main neutrality and retain rare variants, built the observed SFS with a dataset unfiltered for minor allele counts but without outlier SNPs (5,662 SNPs). All individuals were included.\nTo generate SFS\nThe folded SFS for each population was calculated by hand based on the allele frequency of the reference allele for each SNP in each sampling site\nTo calculate the folded allele frequency at each SNP in each sampling site (see Data/allele_freqs_all.xlsx):\n\nIf the reference allele frequency was > 0.5, it was subtracted from 1.\nThe folded allele frequency was then multiplied by 2N (N = sample size of each site).\nThe number of observations of each allele count was then calculated (ex. how many SNPs had folded allele count of 0, 1, 2, 3, etc.). These counts were then used as the SFS in each population.\n\nTo run Stairway Plot\nDownloaded Stairway Plot v.2 from https://github.com/xiaoming-liu/stairway-plot-v2 to Amarel and unzipped/installed in ~/Programs.\nunzip stairway_plot_v2.1.1.zip\nCreated blueprint file (config/input file) to run Stairway Plot for each population. Example for Japan (Japan_fold.blueprint) is below:\n#Japan blueprint file\n#input setting\npopid: Japan_fold # id of the population (no white space)\nnseq: 16 # number of sequences (2N)\nL: 1199553 # total number of observed nucleic sites, including polymorphic and monomorphic\nwhether_folded: true # whether the SFS is folded (true or false)\nSFS: 1434 586 361 289 175 223 208 188 # snp frequency spectrum: number of singleton, number of doubleton, etc. (separated by white space) (should be N/2 values)\n#smallest_size_of_SFS_bin_used_for_estimation: 1 # default is 1; to ignore singletons, uncomment this line and change this number to 2\n#largest_size_of_SFS_bin_used_for_estimation: 15 # default is nseq/2 for folded SFS\npct_training: 0.67 # percentage of sites for training\nnrand: 4        7       11      14 # number of random break points for each try (separated by white space)\nproject_dir: Japan_fold # project directory\nstairway_plot_dir: stairway_plot_es # directory to the stairway plot files\nninput: 200 # number of input files to be created for each estimation\n#random_seed: 6\n#output setting\nmu: 1e-8 # assumed mutation rate per site per generation\nyear_per_generation: 5 # assumed generation time (in years)\n#plot setting\nplot_title: Japan_fold # title of the plot\nxrange: 0.1,100 # Time (1k year) range; format: xmin,xmax; \"0,0\" for default\nyrange: 0,0 # Ne (1k individual) range; format: xmin,xmax; \"0,0\" for default\nxspacing: 2 # X axis spacing\nyspacing: 2 # Y axis spacing\nfontsize: 12 # Font size\nNotes:\n\nL: length of sequence (total number of obesrved nucleic sites both polymorphic and monomorphic). Here, this was the total # of bp in the contigs included in the analysis.\nnrand: # of random break points for each try. Used 4 numbers ((nseq-2)/4, (nseq-2)/2, ((nseq-2)*3)/4, nseq-2), suggested in manual.\nmu: mutation rate per site per generation, same as used in fastsimcoal2.\nyear_per_generation: assumed generation time (years), same as used in fastsimcoal2.\n\nRan Stairway Plot:\n#first make batch file\njava -cp stairway_plot_es Stairbuilder Japan_fold.blueprint\n\n#run batch file\nbash Japan_fold_blueprint.sh\nOnce done, made output folder Japan_fold with diff estimations of demographic histories (one for each of the rand values put in the blueprint file). The Japan_fold_final.summary.png contains the best estimate.\nRepeated for each sampling site. Copied *_fold_final.summary.png to local computer for visualization."
  },
  {
    "objectID": "databases/genomic_repositories/rspb.2021.0407/STRUCTURE_upstream.html",
    "href": "databases/genomic_repositories/rspb.2021.0407/STRUCTURE_upstream.html",
    "title": "Seascape2022",
    "section": "",
    "text": "Code for STRUCTURE analyses run on an HPC cluster (Amarel at Rutgers) before transferring output files to local computer for downstream analyses and visualization in R.\nDownloaded STRUCTURE v.2.3.4 to Amarel and unzipped/installed in ~/Programs.\nNote: Process below is for running STRUCTURE with the full mac2 dataset. For the other datasets, inputs and file names were changed as needed.\nCreated STRUCTURE input file with PGDSpider v.2.1.1.5.\n\nTook output.hicov2.snps.only.mac2.vcf file and converted to STRUCTURE_mac2.txt.\n\nUsed following code to run PGDSpider v.2.1.1.5 on Amarel:\n#grabbed an interactive node with following line of code\nsrun --partition=main --nodes=1 --ntasks=1 --cpus-per-task=1 --mem=2000 --time=00:30:00 --export=ALL --pty bash -i\n\n#called #PGDSpider in ~/Programs/PGDSpider_2.1.1.5\njava -Xmx1024m -Xms512m -jar PGDSpider2-cli.jar -inputfile output.hicov2.snps.only.mac2.vcf -inputformat VCF -outputfile STRUCTURE_mac2.txt -outputformat STRUCTURE -spid VCF_STRUCTURE.spid\n\n#NOTE: if don't have spid file made, or know proper format, running the previous line of code without the -spid argument will generate a template spid file to modify as needed\nSpid format for PGDSpider:\n# VCF Parser questions\nPARSER_FORMAT=VCF\n\n# Only output SNPs with a phred-scaled quality of at least:\nVCF_PARSER_QUAL_QUESTION=\n# Select population definition file:\nVCF_PARSER_POP_FILE_QUESTION= Pop_assignments.txt #txt file with indv names in one column and pop assignment (Pop_1, etc.) in next (no headers, tab-delimited)\n# What is the ploidy of the data?\nVCF_PARSER_PLOIDY_QUESTION=  DIPLOID\n# Do you want to include a file with population definitions?\nVCF_PARSER_POP_QUESTION= TRUE\n# Output genotypes as missing if the phred-scale genotype quality is below:\nVCF_PARSER_GTQUAL_QUESTION=\n# Do you want to include non-polymorphic SNPs?\nVCF_PARSER_MONOMORPHIC_QUESTION= TRUE\n# Only output following individuals (ind1, ind2, ind4, ...):\nVCF_PARSER_IND_QUESTION=\n# Only input following regions (refSeqName:start:end, multiple regions: whitespace separated):\nVCF_PARSER_REGION_QUESTION=\n# Output genotypes as missing if the read depth of a position for the sample is below:\nVCF_PARSER_READ_QUESTION=\n# Take most likely genotype if \"PL\" or \"GL\" is given in the genotype field?\nVCF_PARSER_PL_QUESTION= FALSE\n# Do you want to exclude loci with only missing data?\nVCF_PARSER_EXC_MISSING_LOCI_QUESTION= FALSE\n\n# STRUCTURE Writer questions\nWRITER_FORMAT=STRUCTURE\n\n# Specify the locus/locus combination you want to write to the STRUCTURE file:\nSTRUCTURE_WRITER_LOCUS_COMBINATION_QUESTION=\n# Do you want to include inter-marker distances?\nSTRUCTURE_WRITER_LOCI_DISTANCE_QUESTION=FALSE\n# Specify which data type should be included in the STRUCTURE file  (STRUCTURE can only analyze one data type per file):\nSTRUCTURE_WRITER_DATA_TYPE_QUESTION=SNP\n# Save more specific fastSTRUCTURE format?\nSTRUCTURE_WRITER_FAST_FORMAT_QUESTION=FALSE\nMoved STRUCTURE_mac2.txt to ~/Programs/console.\nCreated STRUCTURE.sbatch file to run STRUCTURE:\n#!/bin/bash\n\n#SBATCH --partition=p_mlp195 #p_mlp195 for pinsky lab partition, otherwise use main (or EOAS or E&E)\n#SBATCH --job-name=STRUCURE\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=8000\n#SBATCH --time=2:00:00\n\n#SBATCH --requeue\n#SBATCH --export=ALL\n#SBATCH --output=slurm.%j.out\n#SBATCH --error=slurm.%j.err\n#SBATCH --mail-type=END\n#SBATCH --mail-user=rene.clark@rutgers.edu\n\n#script for running STRUCTURE on Amarel\n\nstructure -m mainparams_K1 -o mac2_K1_run1\nmainparams_K1 file structure below. Ran with 100,000 burn-in iterations & 10,000 MCMC reps after burn-in:\nKEY PARAMETERS FOR THE PROGRAM structure.  YOU WILL NEED TO SET THESE\nIN ORDER TO RUN THE PROGRAM.  VARIOUS OPTIONS CAN BE ADJUSTED IN THE\nFILE extraparams.\n\n\n\"(int)\" means that this takes an integer value.\n\"(B)\"   means that this variable is Boolean\n        (ie insert 1 for True, and 0 for False)\n\"(str)\" means that this is a string (but not enclosed in quotes!)\n\n\nBasic Program Parameters\n\n#define MAXPOPS    1      // (int) number of populations assumed\n#define BURNIN    100000   // (int) length of burnin period\n#define NUMREPS   10000   // (int) number of MCMC reps after burnin\n\nInput/Output files\n\n#define INFILE   STRUCTURE_mac2.txt   // (str) name of input data file\n#define OUTFILE  mac2_K1_run1.out  //(str) name of output data file\n\nData file format\n\n#define NUMINDS    25    // (int) number of diploid individuals in data file\n#define NUMLOCI    4212    // (int) number of loci in data file\n#define PLOIDY       2    // (int) ploidy of data\n#define MISSING     -9    // (int) value given to missing genotype data\n#define ONEROWPERIND 0    // (B) store data for individuals in a single line\n\n\n#define LABEL     1     // (B) Input file contains individual labels\n#define POPDATA   1     // (B) Input file contains a population identifier\n#define POPFLAG   0     // (B) Input file contains a flag which says\n                              whether to use popinfo when USEPOPINFO==1\n#define LOCDATA   0     // (B) Input file contains a location identifier\n\n#define PHENOTYPE 0     // (B) Input file contains phenotype information\n#define EXTRACOLS 0     // (int) Number of additional columns of data\n                             before the genotype data start.\n\n#define MARKERNAMES      1  // (B) data file contains row of marker names\n#define RECESSIVEALLELES 0  // (B) data file contains dominant markers (eg AFLPs)\n                            // and a row to indicate which alleles are recessive\n#define MAPDISTANCES     0  // (B) data file contains row of map distances\n                            // between loci\n\n\nAdvanced data file options\n\n#define PHASED           0 // (B) Data are in correct phase (relevant for linkage model only)\n#define PHASEINFO        0 // (B) the data for each individual contains a line\n                                  indicating phase (linkage model)\n#define MARKOVPHASE      0 // (B) the phase info follows a Markov model.\n#define NOTAMBIGUOUS  -999 // (int) for use in some analyses of polyploid data\nextraparams file structure (top lines). Ran with the admixture model and correlated allele frequencies:\nEXTRA PARAMS FOR THE PROGRAM structure.  THESE PARAMETERS CONTROL HOW THE\nPROGRAM RUNS.  ATTRIBUTES OF THE DATAFILE AS WELL AS K AND RUNLENGTH ARE\nSPECIFIED IN mainparams.\n\n\"(int)\" means that this takes an integer value.\n\"(d)\"   means that this is a double (ie, a Real number such as 3.14).\n\"(B)\"   means that this variable is Boolean\n        (ie insert 1 for True, and 0 for False).\n\nPROGRAM OPTIONS\n\n#define NOADMIX     0 // (B) Use no admixture model (0=admixture model, 1=no-admix)\n#define LINKAGE     0 // (B) Use the linkage model model\n#define USEPOPINFO  0 // (B) Use prior population information to pre-assign individuals to clusters\n\n#define LOCPRIOR    0 //(B)  Use location information to improve weak data (LOCISPOP = 1)\n\n#define FREQSCORR   1 // (B) allele frequencies are correlated among pops\n\n#everything else left unchanged\nSubmitted job:\nsbatch STRUCTURE.sbatch\nRan 5 replicates of each value of K (1-5). Done for full mac2 dataset, only SNPs in HWE, only non-outlier SNPS, and only outlier SNPs.\nCopied *_f files to local computer and read into R for downstream analyses (Scripts/STRUCTURE.R)."
  },
  {
    "objectID": "databases/genomic_repositories/rspb.2021.0407/TajimasD_upstream.html",
    "href": "databases/genomic_repositories/rspb.2021.0407/TajimasD_upstream.html",
    "title": "Seascape2022",
    "section": "",
    "text": "Code for calculating Tajima’s D using VCFtools. Run on an HPC cluster (Amarel at Rutgers) before transferring output files to local computer for downstream analyses and visualization in R.\nVCFtools v.1.16 was already installed as a module on Amarel. To load in working environment, use following code:\nmodule load VCFtools\nFor Tajima’s D, used a VCF file unfiltered for minor allele count (originally 5,729 SNPs), and sub-setted to only synonymous SNPs (to avoid potential issues with purifying selection in coding & UTR regions). Synonymous SNPs were identified by mapping contigs to the Amphiprion frenatus genome and then performing a structural annotation on these mapped SNPs using SnpEff. The A. frenatus genome is available at https://datadryad.org/stash/dataset/doi:10.5061/dryad.nv1sv.\nTo map contigs\nPerformed BLASTn searches against the A. frenatus fasta. Each SNP was mapped to its corresponding bp and contig in A. frenatus, and these coordinates were used to create a new VCF file for input into SnpEff.\nTo run SnpEff\nSnpEff was downloaded and unzipped in ~/Programs. As A.frenatus is a non-model organism, had to create a new database from the afrenatus.all_contigs.onlystandardFiltered.gff3 file (downloaded from Dryad).\nAdded database to ~/Programs/snpEff/snpEff.config:\n#in Non-standard Databases section of config file, added following lines:\n\n# Amphiprion frenatus\naf1.genome : Amphiprion frenatus\nBuilt database in ~/Programs/snpEff/data:\nmkdir af1\ncd ~/Programs/snpEff/data/af1\n\n#copied afrenatus.all_contigs.onlystandardFiltered.gff3 here and renamed\nmv afrenatus.all_contigs.onlystandardFiltered.gff3 genes.gff\n\n#copied Amphiprion_frenatus_GenomeAssembly.fasta here and renamed\nmv Amphiprion_frenatus_GenomeAssembly.fasta sequences.fa\n\n#created database\ncd ~/Programs/snpEff\njava -jar snpEff.jar build -gff3 -v af1 #works if creates snpEffectPredictor.bin in data/af1\nCreated snp.sbatch file to run SnpEff:\n#!/bin/bash\n\n#SBATCH --partition=p_mlp195 #p_mlp195 for pinsky lab partition, otherwise use main (or EOAS or E&E)\n#SBATCH --job-name=snpeff\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=8000\n#SBATCH --time=2:00:00\n\n#SBATCH --requeue\n#SBATCH --export=ALL\n#SBATCH --output=slurm.%j.out\n#SBATCH --error=slurm.%j.err\n#SBATCH --mail-type=END\n#SBATCH --mail-user=rene.clark@rutgers.edu\n\n#script for running snpEff on Amarel\n\njava -Xmx8g -jar snpEff.jar af1 data/af1/output.hicov2.snps.only_afrenatus_FULL.vcf > afrenatus_ann_FULL.vcf\nSubmitted job:\nsbatch snpEff.sbatch\nCreates three files:\n\nafrenatus_ann_FULL.vcf: vcf file with the structural annotation included.\nsnpEff_genes_FULL.txt: breakdown of # and type of SNPs by contig.\nsnpEff_summary_FULL.html: summary of # and type of SNPs in entire VCF\n\nTook afrenatus_ann_FULL.vcf, converted back to A. clarkii bp & contig coordinates, and subsetted to only synonymous SNPs.\nTo calculate Tajima’s D\nFirst, created sub-setted VCF file with only synonymous SNPs:\n#created synonymous_SNPs.txt (column w/contig & column w/bp of all synonymous SNPs)\n\n#used VCFtools to filter output.hicov2.snps.only.mac1.vcf to only include synonymous SNPs\nvcftools --vcf output.hicov2.snps.only.mac1.vcf --positions synonymous_SNPs.txt --recode --recode-INFO-all --out output.hicov2.snps.only.mac1.synonymous\nThen, sub-setted output.hicov2.snps.only.mac1.synonymous.vcf for each sampling site (code for Japan below):\n#created J_individuals.txt (column of individuals in Japan)\n\n#used VCFtools to filter output.hicov2.snps.only.mac1.synonymous.vcf to only include individuals from Japan\nvcftools --vcf output.hicov2.snps.only.mac1.synonymous.vcf --keep J_individuals.txt --recode --recode-INFO-all --out Jmac1_SYN\n\n#repeated for Philippines & Indonesia\nCalculated Tajima’s D, both within each population and with the full dataset (all individuals included in VCF):\n#NOTE: don't grab interactive node for VCFtools jobs bc run in seconds, BUT if is more data-intensive, should get off log-in node to do this.\n\nvcftools --vcf Jmac1_SYN.vcf --TajimaD 10000 --out SYN_mac1_JTajimasD\n\n#used bins of 10,000 bp so would calculate across each transcript individually (largest transcript <7000 bp)\n#repeated for Philippines, Indonesia, and the original vcf file w/all individuals\nCopied *Tajima.D files to local computer and opened in Excel to create .csv files that were then read into R for downstream analyses (Scripts/TajimaD_script.R)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fronteras de la Ciencia 2022",
    "section": "",
    "text": "To learn more about Quarto websites visit https://quarto.org/docs/websites.\n\nAcerca del proyecto\nDescripción de la busqueda bibliográfica\nCaracteriación general de los metadatos\nCaracterización ambiental"
  }
]