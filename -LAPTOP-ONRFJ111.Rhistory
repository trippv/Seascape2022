temp
plot(temp)
sum(temp)
temp <- !is.na(temp)
temp
temp <- temp[!is.na(temp)]
temp <- ncdf4::ncvar_get(nc)
temp <- temp[!is.na(temp)]
API_single_point <- ' python -m motuclient --motu https://my.cmems-du.eu/motu-web/Motu --service-id GLOBAL_REANALYSIS_PHY_001_031-TDS --product-id global-reanalysis-phy-001-031-grepv2-mnstd-daily --longitude-min 112.618 --longitude-max 112.618 --latitude-min 24.686 --latitude-max 24.686 --date-min "2018-01-01 00:00:00" --date-max "2019-12-31 00:00:00"  --depth-min 0.5057 --depth-max 0.5058  --variable thetao_mean  --out-dir <OUTPUT_DIRECTORY> --out-name <OUTPUT_FILENAME> --user <USERNAME> --pwd <PASSWORD>'
get_cmems_from_api(API = API_single_point,
PYTHON = python_path,
NEW_COORDS = FALSE,
NAME_TAG = "somewhere",
OUTDIR = ".",
USER = "mtrippvaldez",
PWD = "Esper4nz")
nc <- ncdf4::nc_open("global-reanalysis-phy-001-031-grepv2-mnstd-daily_somewhere")
print(nc)
temp <- ncdf4::ncvar_get(nc)
temp
API_single_point <- ' python -m motuclient --motu https://my.cmems-du.eu/motu-web/Motu --service-id GLOBAL_REANALYSIS_PHY_001_031-TDS --product-id global-reanalysis-phy-001-031-grepv2-mnstd-daily --longitude-min -112.618 --longitude-max -112.618 --latitude-min 24.686 --latitude-max 24.686 --date-min "2018-01-01 00:00:00" --date-max "2019-12-31 00:00:00"  --depth-min 0.5057 --depth-max 0.5058  --variable thetao_mean  --out-dir <OUTPUT_DIRECTORY> --out-name <OUTPUT_FILENAME> --user <USERNAME> --pwd <PASSWORD>'
get_cmems_from_api(API = API_single_point,
PYTHON = python_path,
NEW_COORDS = FALSE,
NAME_TAG = "somewhere",
OUTDIR = ".",
USER = "mtrippvaldez",
PWD = "Esper4nz")
nc <- ncdf4::nc_open("global-reanalysis-phy-001-031-grepv2-mnstd-daily_somewhere")
print(nc)
temp <- ncdf4::ncvar_get(nc)
temp
plot(temp)
API_single_point <- ' python -m motuclient --motu https://my.cmems-du.eu/motu-web/Motu --service-id GLOBAL_REANALYSIS_PHY_001_031-TDS --product-id global-reanalysis-phy-001-031-grepv2-mnstd-daily --longitude-min -112.618 --longitude-max -112.618 --latitude-min 24.686 --latitude-max 24.686 --date-min "2001-01-01 00:00:00" --date-max "2019-12-31 00:00:00"  --depth-min 0.5057 --depth-max 0.5058  --variable thetao_mean  --out-dir <OUTPUT_DIRECTORY> --out-name <OUTPUT_FILENAME> --user <USERNAME> --pwd <PASSWORD>'
get_cmems_from_api(API = API_single_point,
PYTHON = python_path,
NEW_COORDS = FALSE,
NAME_TAG = "somewhere",
OUTDIR = ".",
USER = "mtrippvaldez",
PWD = "Esper4nz")
nc <- ncdf4::nc_open("global-reanalysis-phy-001-031-grepv2-mnstd-daily_somewhere")
print(nc)
temp <- ncdf4::ncvar_get(nc)
plot(temp)
library(here)
DOI_list <- list.dirs(path = here("databases/genomic_repositories/"),
full.names = FALSE, recursive = FALSE)
metadata <- read_csv(file = here("output/tables/MAtriz_metadatos_backup.csv"))
metadata$DOI_ID <- gsub(".*\\/", "", metadata$DOI)
library(here)
library(tidyverse)
DOI_list <- list.dirs(path = here("databases/genomic_repositories/"),
full.names = FALSE, recursive = FALSE)
metadata <- read_csv(file = here("output/tables/MAtriz_metadatos_backup.csv"))
metadata <- read_csv(file = here("output/tables/MAtriz_metadatos_backup.csv"))
metadata$DOI_ID <- gsub(".*\\/", "", metadata$DOI)
metadata_filtrado <- metadata %>%
# filter DOI from selected list
filter(DOI_ID %in% DOI_list) %>%
#select specific columns to display
select(DOI_ID, Species, Generic, `World region`, Country, `Environmental association`, `Main environemntal driver adaptive`)
coordinates_all <- data.frame()
for (i in 1:length(DOI_list)) {
# 1. read each file
file_dt <- read.table(here("databases", "coordinates", paste0(DOI_list[i], ".txt")),
sep = "\t",header = TRUE)
# Check colnames to convert DMS to DEC degrees if necesary
ifelse(any(grepl("dec", colnames(file_dt))),
#if colnames says lats are in DEC, no convertion
lats <- file_dt$latitude_dec,
#otherwise, convert with sp package
lats <- sp::char2dms(file_dt$latitude_dms,
chd = "d", chm = "m", chs="s") %>%
as.numeric()
)
# convert longitude
ifelse(any(grepl("dec", colnames(file_dt))),
#if colnames says lats are in DEC, no convertion
lons <- file_dt$longitude_dec,
#otherwise, convert with sp package
lons <- sp::char2dms(file_dt$longitude_dms,
chd = "d", chm = "m", chs="s") %>%
as.numeric()
)
# if sample column is present, add it to the table, otherwise add NAs
ifelse("sample" %in% colnames(file_dt),
samples <- file_dt$sample,
samples <- rep("NA", nrow(file_dt)))
#if region is present, add it to the table, otherwise add NAs
ifelse("region" %in% colnames(file_dt),
region <- file_dt$region,
region <- rep("NA", nrow(file_dt)))
# if site name is present, add it to the table, otherwise add NAs
ifelse("site" %in% colnames(file_dt),
site <- file_dt$site,
site <- rep("NA", nrow(file_dt)))
# if site code is present, add it to the table, otherwise add NAs
ifelse("code" %in% colnames(file_dt),
code <- file_dt$code,
code <- rep("NA", nrow(file_dt)))
# set values for table
latitude <- as.numeric(lats)
longitude <- as.numeric(lons)
DOI <- rep(as.character(DOI_list[i]), nrow(file_dt))
N <- samples
# data frame with values
output <- data.frame(DOI = DOI,
latitude = latitude,
longitude = longitude,
N = samples,
region = region,
code = code,
site = site
)
# merge table with previous in loop
coordinates_all <- rbind(coordinates_all, output)
}
plot_raster_stat <- function(x, stat = "stat", variable = variable){
# convert raster to df
nc_df <- raster::as.data.frame(x, xy=TRUE, na.rm=TRUE)
### fitler the coordinate data to the map area
nc_lats <- range(nc_df$y)
nc_lons <- range(nc_df$x)
coords_map <- coordinates_all %>%
filter(between(latitude, min(nc_lats), max(nc_lats)),
between(longitude, min(nc_lons), max(nc_lons)))
# labels for plotting
label_name <- ifelse(variable == "temperature", "SST (Â°C)", variable)
caption <- basename(ncfile)
caption <- gsub("(-monthly).*", "", caption)
# plot
map_plot <- ggplot()+
# add raster layer
geom_raster(aes(x=x, y=y, fill=layer), data=nc_df) +
# define color palette of raster layer
scale_fill_distiller(palette = "Spectral", name = label_name) +
# add countries layers
geom_sf(fill=grey(0.9), color=grey(0.6), lwd = 0.1, data=countries) +
# define spatial extent
coord_sf(xlim = range(nc_df$x), ylim = range(nc_df$y), expand = F, ndiscr = 500) +
# Add points
geom_point(data = coords_map,
aes(x = longitude, y = latitude, color = DOI), size = 3)+
# labels
labs(title = paste0(variable," ", stat),
caption = paste0("Data:", caption),
x = "Longitude",
y = "Latitude") +
# theme
theme_bw()
# add map name
map_name <- str_remove(basename(ncfile), ".nc")
## save plot
ggsave(plot = map_plot,
filename = here("output/maps/environment", variable,
paste0(map_name,"_",stat, ".png")),
dpi = 300,
device = "png")
# print map
print(map_plot)
}
# import countries layer from Natural Earth
library(rnaturalearth)
# import countries layer from Natural Earth
library(rnaturalearth)
library(rnaturalearthdata)
countries <- ne_countries(scale = "medium", returnclass = "sf")
variable <- "sea_surface_temperature"
ncfile_list <- list.files(path = here("databases/environment/cmems", variable),
full.names = FALSE)
map = 1
# Set the path for the netcdf
ncfile <- here("databases/environment/cmems",variable, ncfile_list[map])
# Import NetCDF
nc <- ncdf4::nc_open(ncfile)
### Multiband
# import multi-band NetCDF file
nc_multi <- raster::brick(ncfile)
# calculate statistic: mean value
nc_mean <- raster::calc(nc_multi, fun = mean)
# calculate statistic: standard deviation
nc_sd <- raster::calc(nc_multi, fun = sd)
# calculate statistic: amplitude (max - min)
nc_range <- raster::calc(nc_multi, fun = function(x) {max(x) - min(x)})
# calculate statistic: kurtosis (from moments package)
nc_kurtosis <- raster::calc(nc_multi, fun = function(x) {moments::kurtosis(x)})
## Use plot function to plot and save each of the maps
plot_raster_stat(nc_mean, stat = "Mean", variable = variable)
#### New function
source("scripts/functions/get_cmems_from_api.R")
API_single_point <- ' python -m motuclient --motu https://my.cmems-du.eu/motu-web/Motu --service-id GLOBAL_REANALYSIS_PHY_001_031-TDS --product-id global-reanalysis-phy-001-031-grepv2-mnstd-daily --longitude-min -112.618 --longitude-max -112.618 --latitude-min 24.686 --latitude-max 24.686 --date-min "2001-01-01 00:00:00" --date-max "2019-12-31 00:00:00"  --depth-min 0.5057 --depth-max 0.5058  --variable thetao_mean  --out-dir <OUTPUT_DIRECTORY> --out-name <OUTPUT_FILENAME> --user <USERNAME> --pwd <PASSWORD>'
API_single_point <- ' python -m motuclient --motu https://my.cmems-du.eu/motu-web/Motu --service-id GLOBAL_REANALYSIS_PHY_001_031-TDS --product-id global-reanalysis-phy-001-031-grepv2-mnstd-daily --longitude-min -112.618 --longitude-max -112.618 --latitude-min 24.686 --latitude-max 24.686 --date-min "1999-01-01 00:00:00" --date-max "2019-12-31 00:00:00"  --depth-min 0.5057 --depth-max 0.5058  --variable thetao_mean  --out-dir <OUTPUT_DIRECTORY> --out-name <OUTPUT_FILENAME> --user <USERNAME> --pwd <PASSWORD>'
get_cmems_from_api(API = API_single_point,
PYTHON = python_path,
NEW_COORDS = FALSE,
NAME_TAG = "somewhere",
OUTDIR = ".",
USER = "mtrippvaldez",
PWD = "Esper4nz")
# Path to Python in windows
python_path <- "C:/Users/migue/AppData/Local/Programs/Python/Python310/python"
get_cmems_from_api(API = API_single_point,
PYTHON = python_path,
NEW_COORDS = FALSE,
NAME_TAG = "somewhere",
OUTDIR = ".",
USER = "mtrippvaldez",
PWD = "Esper4nz")
library(ncdf4)
nc <- ncdf4::nc_open("global-reanalysis-phy-001-031-grepv2-mnstd-daily_somewhere")
print(nc)
temp <- ncdf4::ncvar_get(nc)
plot(temp)
plot(temp, type = "l")
## Create a daily Date object - helps my work on dates
inds <- seq(as.Date("1991-01-01"), as.Date("2019-12-31"), by = "day")
inds
## Create a daily Date object - helps my work on dates
inds <- seq(as.Date("1999-01-01"), as.Date("2019-12-31"), by = "day")
inds
myts <- ts(temp,     # random data
start = c(1991, as.numeric(format(inds[1], "%j"))),
frequency = 365)
myts
plot(myts)
mty_decom <- decompose(myts)
mty_decom
plot(mty_decom)
hist(temp)
moments::skewness(temp)
moments::kurtosis(temp)
moments::skewness(mty_decom$random)
moments::skewness(mty_decom$random, na.rm = TRUE)
moments::kurtosis(mty_decom$random, na.rm = TRUE)
hist(mty_decom$random)
hist(mty_decom$seasonal)
hist(mty_decom$trend)
hist(mty_decom$seasonal)
hist(mty_decom$random)
# The packages we will need
# install.packages("dplyr")
install.packages("lubridate")
# install.packages("ggplot2")
install.packages("tidync")
install.packages("doParallel")
install.packages("rerddap")
# The packages we will use
library(dplyr) # A staple for modern data management in R
library(lubridate) # Useful functions for dealing with dates
library(ggplot2) # The preferred library for data visualisation
library(tidync) # For easily dealing with NetCDF data
library(rerddap) # For easily downloading subsets of data
library(doParallel) # For parallel processing
# First we tell R where the data are on the interwebs
OISST_base_url <- "https://www.ncei.noaa.gov/data/sea-surface-temperature-optimum-interpolation/v2.1/access/avhrr/"
OISST_base_url
# Now we create a data.frame that contains all of the dates we want to download
# NB: In order to change the dates download changes the dates in the following line
OISST_dates <- data.frame(t = seq(as.Date("2019-12-01"), as.Date("2019-12-31"), by = "day"))
OISST_dates
# To finish up this step we add some text to those dates so they match the OISST file names
OISST_files <- OISST_dates %>%
mutate(t_day = gsub("-", "", t),
t_month = substr(t_day, 1, 6),
t_year = year(t),
file_name = paste0(OISST_base_url, t_month, "/", "oisst-avhrr-v02r01.", t_day ,".nc"))
OISST_files
dir.create("databases/environment/OISST", showWarnings = F)
# This function will go about downloading each day of data as a NetCDF file
# Note that this will download files into a 'data/OISST' folder in the root directory
# If this folder does not exist it will create it
# If it does not automatically create the folder it will need to be done manually
# The folder that is created must be a new folder with no other files in it
# A possible bug with netCDF files in R is they won't load correctly from
# existing folders with other file types in them
# This function will also check if the file has been previously downloaded
# If it has it will not download it again
OISST_url_daily_dl <- function(target_URL){
dir.create("databases/environment/OISST", showWarnings = F)
file_name <- paste0("databases/environment/OISST/",sapply(strsplit(target_URL, split = "/"), "[[", 10))
if(!file.exists(file_name)) download.file(url = target_URL, method = "libcurl", destfile = file_name)
}
dir.create("databases/environment/OISST", showWarnings = T)
# This function will go about downloading each day of data as a NetCDF file
# Note that this will download files into a 'data/OISST' folder in the root directory
# If this folder does not exist it will create it
# If it does not automatically create the folder it will need to be done manually
# The folder that is created must be a new folder with no other files in it
# A possible bug with netCDF files in R is they won't load correctly from
# existing folders with other file types in them
# This function will also check if the file has been previously downloaded
# If it has it will not download it again
OISST_url_daily_dl <- function(target_URL){
dir.create("databases/environment/OISST", showWarnings = T)
file_name <- paste0("databases/environment/OISST/",sapply(strsplit(target_URL, split = "/"), "[[", 10))
if(!file.exists(file_name)) download.file(url = target_URL, method = "libcurl", destfile = file_name)
}
# The more cores used, the faster the data may be downloaded
# It is best practice to not use all of the cores on one's machine
# The laptop on which I am running this code has 8 cores, so I use 7 here
doParallel::registerDoParallel(cores = 3)
# And with that we are clear for take off
system.time(plyr::l_ply(OISST_files$file_name, .fun = OISST_url_daily_dl, .parallel = T)) # ~15 seconds
# visualize data
nc_file <- ncdf4::nc_open(list.files("databases/environment/OISST/")[1])
list.files("databases/environment/OISST/")
# visualize data
nc_file <- ncdf4::nc_open(list.files("databases/environment/OISST/", full.names = TRUE)[1])
list.files("databases/environment/OISST/", full.names = TRUE)[1]
list.files(here::here("databases/environment/OISST"), full.names = TRUE)
# visualize data
nc_file <- ncdf4::nc_open(list.files(here::here("databases/environment/OISST"), full.names = TRUE)[1])
# visualize data
nc_files <- list.files(here::here("databases/environment/OISST"), full.names = TRUE)
nc_files
nc <- ncdf4::nc_open(nc_files[1])
nc <- ncdf4::nc_open(nc_files[1])
ncfile <- "databases/environment/OISST/oisst-avhrr-v02r01.20191201.nc"
# Load package
library(ncdf4)
library(raster)
library(sf)
# Import NetCDF
nc <- nc_open(ncfile)
# Print information about the NetCDF file
print(nc)
# visualize data
nc_files <- list.files(here::here("databases/environment/OISST"), full.names = TRUE)
nc_files
nc <- ncdf4::nc_open(nc_files[1])
# Print information about the NetCDF file
print(nc)
values <- ncvar_get(nc)
values
values <- ncdf4::ncvar_get(nc)
# Print information about the NetCDF file
print(nc)
# import NetCDF with raster
sst_single <- raster(ncfile)
# import NetCDF with raster
sst_single <- raster(nc_files)
# import NetCDF with raster
sst_single <- raster(nc_files,varname = "sst")
# print a summary of the raster
sst_single
# plot raster dataset
plot(sst_single)
OISST_files
OISST_files
OISST_files$file_name
# And with that we are clear for take off
system.time(plyr::l_ply(OISST_files$file_name, .fun = OISST_url_daily_dl, .parallel = F)) # ~15 seconds
# visualize data
nc_files <- list.files(here::here("databases/environment/OISST"), full.names = TRUE)
nc_files
nc <- ncdf4::nc_open(nc_files[1])
# Print information about the NetCDF file
print(nc)
# import NetCDF with raster
sst_single <- raster(nc_files[2],varname = "sst")
nc <- ncdf4::nc_open(nc_files[2])
OISST_files
# import NetCDF with raster
sst_single <- raster(nc_files[31],varname = "sst")
OISST_files
# First we tell R where the data are on the interwebs
OISST_base_url <- "https://www.ncei.noaa.gov/data/sea-surface-temperature-optimum-interpolation/v2.1/access/avhrr/"
# Now we create a data.frame that contains all of the dates we want to download
# NB: In order to change the dates download changes the dates in the following line
OISST_dates <- data.frame(t = seq(as.Date("2019-12-01"), as.Date("2019-12-31"), by = "day"))
# To finish up this step we add some text to those dates so they match the OISST file names
OISST_files <- OISST_dates %>%
mutate(t_day = gsub("-", "", t),
t_month = substr(t_day, 1, 6),
t_year = year(t),
file_name = paste0(OISST_base_url, t_month, "/", "oisst-avhrr-v02r01.", t_day ,".nc"))
# The packages we will use
library(dplyr) # A staple for modern data management in R
library(lubridate) # Useful functions for dealing with dates
library(ggplot2) # The preferred library for data visualisation
library(tidync) # For easily dealing with NetCDF data
library(rerddap) # For easily downloading subsets of data
library(doParallel) # For parallel processing
dir.create("databases/environment/OISST", showWarnings = T)
file_name <- paste0("databases/environment/OISST/",sapply(strsplit(target_URL, split = "/"), "[[", 10))
# First we tell R where the data are on the interwebs
OISST_base_url <- "https://www.ncei.noaa.gov/data/sea-surface-temperature-optimum-interpolation/v2.1/access/avhrr/"
# Now we create a data.frame that contains all of the dates we want to download
# NB: In order to change the dates download changes the dates in the following line
OISST_dates <- data.frame(t = seq(as.Date("2019-12-01"), as.Date("2019-12-31"), by = "day"))
# To finish up this step we add some text to those dates so they match the OISST file names
OISST_files <- OISST_dates %>%
mutate(t_day = gsub("-", "", t),
t_month = substr(t_day, 1, 6),
t_year = year(t),
file_name = paste0(OISST_base_url, t_month, "/", "oisst-avhrr-v02r01.", t_day ,".nc"))
OISST_files
# visualize data
nc_files <- list.files(here::here("databases/environment/OISST"), full.names = TRUE)
nc <- ncdf4::nc_open(nc_files[2])
nc <- ncdf4::nc_open(nc_files[1])
# Print information about the NetCDF file
print(nc)
values <- ncdf4::ncvar_get(nc)
values
# import NetCDF with raster
sst_single <- raster(nc_files[1],varname = "sst")
# import NetCDF with raster
sst_single <- raster::raster(nc_files[1],varname = "sst")
# print a summary of the raster
sst_single
# plot raster dataset
plot(sst_single)
# plot raster dataset
plot(sst_single$Daily.Sea.Surface.Temperature)
# plot raster dataset
plot(sst_single@z)
# import NetCDF with raster
sst_single <- raster::raster(nc_files[1])
# print a summary of the raster
sst_single
# plot raster dataset
plot(sst_single)
# import NetCDF with raster
sst_single <- raster::raster(nc)
# import multi-band NetCDF file
sst_multi <- brick(nc_files[1])
# import multi-band NetCDF file
sst_multi <- raster::brick(nc_files[1])
# print a summary of the brick
sst_multi
# plot brick dataset
levelplot(sst_multi)
# plot brick dataset
raster::levelplot(sst_multi)
sst_mean <- calc(sst_multi, fun = mean)
sst_mean <- raster::calc(sst_multi, fun = mean)
sst_mean <- rotate(sst_mean)
sst_mean <- raster::rotate(sst_mean)
# plot raster dataset
raster::plot(sst_mean, main = "Average SST")
sst_multi
sst_multi@data$X1981.09.01
plot(sst_multi@data$X1981.09.01)
plot(sst_multi@data$X1981.09.01[1,1,])
# The packages we will use
library(dplyr) # A staple for modern data management in R
library(lubridate) # Useful functions for dealing with dates
library(ggplot2) # The preferred library for data visualisation
library(tidync) # For easily dealing with NetCDF data
library(rerddap) # For easily downloading subsets of data
library(doParallel) # For parallel processing
# First we tell R where the data are on the interwebs
OISST_base_url <- "https://www.ncei.noaa.gov/data/sea-surface-temperature-optimum-interpolation/v2.1/access/avhrr/"
# Note that one may go to this URL in any web browser to manually inspect the files
# Now we create a data.frame that contains all of the dates we want to download
# NB: In order to change the dates download changes the dates in the following line
OISST_dates <- data.frame(t = seq(as.Date("2019-12-01"), as.Date("2019-12-31"), by = "day"))
# To finish up this step we add some text to those dates so they match the OISST file names
OISST_files <- OISST_dates %>%
mutate(t_day = gsub("-", "", t),
t_month = substr(t_day, 1, 6),
t_year = year(t),
file_name = paste0(OISST_base_url, t_month, "/", "oisst-avhrr-v02r01.", t_day ,".nc"))
download.file(url = OISST_files$file_name)
download.file(url = OISST_files$file_name, destfile = "test.nc")
OISST_files$file_name
download.file(url = OISST_files$file_name[1], destfile = "test.nc")
nc <- ncdf4::nc_open("test.nc")
download.file(url = https://psl.noaa.gov/thredds/catalog/Datasets/noaa.oisst.v2.highres/catalog.html?dataset=Datasets/noaa.oisst.v2.highres/sst.day.mean.2021.nc, destfile = "test.nc")
download.file(url = "https://psl.noaa.gov/thredds/catalog/Datasets/noaa.oisst.v2.highres/catalog.html?dataset=Datasets/noaa.oisst.v2.highres/sst.day.mean.2021.nc", destfile = "test.nc")
download.file(url = "https://psl.noaa.gov/thredds/fileServer/Datasets/noaa.oisst.v2.highres/sst.day.mean.2021.v2.nc", destfile = "test.nc")
download.file(url = "https://psl.noaa.gov/thredds/fileServer/Datasets/noaa.oisst.v2.highres/sst.day.mean.2021.v2.nc", destfile = "test.nc")
options(timeout=240)
download.file(url = "https://psl.noaa.gov/thredds/fileServer/Datasets/noaa.oisst.v2.highres/sst.day.mean.2021.v2.nc", destfile = "test.nc")
# import NetCDF with raster
sst_single <- raster::raster(here("test.nc"))
# import NetCDF with raster
sst_single <- raster::raster(here::here("test.nc"))
nc <- ncdf4::nc_open("test.nc")
download.file(url = "https://psl.noaa.gov/thredds/fileServer/Datasets/noaa.oisst.v2.highres/sst.day.mean.2021.v2.nc", destfile = "test.nc", method = "wget")
download.file(url = "https://psl.noaa.gov/thredds/fileServer/Datasets/noaa.oisst.v2.highres/sst.day.mean.2021.v2.nc", destfile = "test.nc", method = "wininet")
nc <- ncdf4::nc_open("test.nc")
nc <- ncdf4::nc_open("test.nc", return_on_error = TRUE)
nc <- ncdf4::nc_open("sst.day.mean.2021.v2.nc", return_on_error = TRUE)
# Print information about the NetCDF file
print(nc)
# import NetCDF with raster
sst_single <- raster::raster(here::here("sst.day.mean.2021.v2.nc"))
# print a summary of the raster
sst_single
# plot raster dataset
plot(sst_single)
# import multi-band NetCDF file
sst_multi <- raster::brick(sst.day.mean.2021.v2.nc)
# print a summary of the brick
sst_multi
# import multi-band NetCDF file
sst_multi <- raster::brick("sst.day.mean.2021.v2.nc")
# print a summary of the brick
sst_multi
# plot brick dataset
raster::levelplot(sst_multi)
sst_mean <- raster::calc(sst_multi, fun = mean)
sst_mean <- raster::rotate(sst_mean)
# plot raster dataset
raster::plot(sst_mean, main = "Average SST")
mnt_c <- "sudo mount -t drvfs G: /mnt/g"
system2("bash", mnt_c)
knitr::opts_chunk$set(echo = TRUE)
system(mnt_c)
